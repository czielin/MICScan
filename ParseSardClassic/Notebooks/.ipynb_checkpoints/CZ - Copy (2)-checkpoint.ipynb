{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import JSON from dataset file that is output by the C# application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 9-10: malformed \\N character escape (<ipython-input-2-9de44827df77>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-9de44827df77>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    with open('..\\Output\\NumbersOnlyRegexDatasetWithSource.json') as json_file:\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 9-10: malformed \\N character escape\n"
     ]
    }
   ],
   "source": [
    "with open('..\\Output\\NumbersOnlyRegexDatasetWithSource.json') as json_file:\n",
    "    data = np.array(json.load(json_file))\n",
    "    shuffle = np.random.permutation(np.arange(len(data)))\n",
    "    data = data[shuffle]\n",
    "    for test_case_index in range (0, 5):\n",
    "        print('Class Name:', data[test_case_index]['ClassName'])\n",
    "        print('Is Flawed:', data[test_case_index]['IsFlawed'])\n",
    "        print(type(data[test_case_index]['IsFlawed']))\n",
    "        print('Features:', data[test_case_index]['Features'])\n",
    "        print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "categories = ['No Flaw', 'CWE022', 'CWE078', 'CWE089', 'CWE090', 'CWE091']\n",
    "labels = []\n",
    "examples = []\n",
    "examples_skipped = 0\n",
    "\n",
    "for test_case_index in range(0, len(data)):\n",
    "    if data[test_case_index]['IsFlawed']:\n",
    "        try:\n",
    "            category_index = categories.index(data[test_case_index]['ClassName'])\n",
    "            labels.append(category_index)\n",
    "            examples.append(data[test_case_index]['Features']) \n",
    "        except ValueError:\n",
    "            examples_skipped = examples_skipped + 1\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        examples.append(data[test_case_index]['Features'])\n",
    "        \n",
    "print('Label Count:', len(labels))\n",
    "print('Example Count:', len(examples))\n",
    "print('Examples Skipped:', examples_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for test_case_index in range (0, 5):\n",
    "    print('Label:', labels[test_case_index])\n",
    "    print('Category:', categories[labels[test_case_index]])\n",
    "    print('Example:', examples[test_case_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = int(len(examples) * 0.2)\n",
    "print(\"num_test:\", num_test)\n",
    "train_data, train_labels = examples[num_test:], labels[num_test:]\n",
    "dev_data, dev_labels = examples[:num_test], labels[:num_test]\n",
    "print(len(dev_data))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w[\\w\\.(),]+\\b\",stop_words=None)\n",
    "feature_vectors = vectorizer.fit_transform(train_data)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "dev_vectorizer = CountVectorizer(vocabulary=vocabulary,token_pattern=r\"(?u)\\b\\w[\\w\\.(),]+\\b\",stop_words=None)\n",
    "dev_feature_vectors = dev_vectorizer.fit_transform(dev_data)\n",
    "\n",
    "# K Nearest Neighbors\n",
    "n_neighbors = {'n_neighbors': [1, 3, 5, 7, 9, 11]}\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid=n_neighbors, cv=5)\n",
    "grid_search.fit(feature_vectors, train_labels)\n",
    "print(\"K Nearest Neighbors\")\n",
    "print(\"Optimal value for k:\", grid_search.best_params_)\n",
    "predicted_labels = grid_search.best_estimator_.predict(dev_feature_vectors)\n",
    "print(\"F1 score:\", metrics.f1_score(dev_labels, predicted_labels, average='weighted'))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(dev_labels, predicted_labels))\n",
    "print()\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0] }\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid=alphas, cv=5)\n",
    "grid_search.fit(feature_vectors, train_labels)\n",
    "print(\"Multinomial Naive Bayes\")\n",
    "print(\"Optimal value for alpha:\", grid_search.best_params_)\n",
    "predicted_labels = grid_search.best_estimator_.predict(dev_feature_vectors)\n",
    "print(\"F1 score:\", metrics.f1_score(dev_labels, predicted_labels, average='weighted'))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(dev_labels, predicted_labels))\n",
    "print()\n",
    "\n",
    "# Logistic Regression\n",
    "c_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 500.0, 1000.0]\n",
    "c = {'C': c_values }\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid=c, cv=5)\n",
    "grid_search.fit(feature_vectors, train_labels)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Optimal value for C:\", grid_search.best_params_)\n",
    "predicted_labels = grid_search.best_estimator_.predict(dev_feature_vectors)\n",
    "print(\"F1 score:\", metrics.f1_score(dev_labels, predicted_labels, average='weighted'))\n",
    "print(\"Accuracy: \", metrics.accuracy_score(dev_labels, predicted_labels))\n",
    "print()\n",
    "\n",
    "matrix = confusion_matrix(dev_labels, predicted_labels)\n",
    "print(matrix)\n",
    "max_errors = 0\n",
    "most_confused_actual = -1\n",
    "most_confused_predicted = -1\n",
    "# Find the most confused digit pair\n",
    "for actual in range(0, len(categories)):\n",
    "    for predicted in range(0, len(categories)):\n",
    "        if (actual != predicted and matrix[actual, predicted] > max_errors):\n",
    "            max_errors = matrix[actual, predicted]\n",
    "            most_confused_actual = actual\n",
    "            most_confused_predicted = predicted\n",
    "print (\"Most confused pair is actual =\", categories[most_confused_actual], \", predicted =\", categories[most_confused_predicted])\n",
    "print (\"This error occurred\", max_errors, \"times\")\n",
    "# Print out examples of the confused digits\n",
    "error_pair_count = 0\n",
    "for index in range(0, len(dev_labels)):\n",
    "    if (dev_labels[index] == most_confused_actual and predicted_labels[index] == most_confused_predicted and error_pair_count < 10):\n",
    "        print(dev_data[index])\n",
    "        print(\"-----------------------------\")\n",
    "        error_pair_count = error_pair_count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_table(vectorizer):\n",
    "    feature_vectors = vectorizer.fit_transform(train_data)\n",
    "    c_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "    c = {'C': c_values }\n",
    "    grid_search = GridSearchCV(LogisticRegression(), param_grid=c, cv=5)\n",
    "    grid_search.fit(feature_vectors, train_labels)\n",
    "    max_values = [None]*len(grid_search.best_estimator_.coef_)\n",
    "    for class_index in range(0, len(grid_search.best_estimator_.coef_)):\n",
    "        top_5 = np.argsort(grid_search.best_estimator_.coef_[class_index])[-5:]\n",
    "        max_values[class_index] = top_5\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    #print('{:<18}'.format(newsgroups_train.target_names[class_index]), '{:<20}'.format(weight), '{:<11}'.format(feature_names[feature_index])) \n",
    "    for class_index in range(0, len(grid_search.best_estimator_.coef_)):\n",
    "        for feature_index in max_values[class_index]:\n",
    "            weight = grid_search.best_estimator_.coef_[class_index][feature_index]\n",
    "            print('{:<19}'.format(categories[class_index]), end = '')\n",
    "            print('{:<20}'.format(feature_names[feature_index]), end = '')\n",
    "            print('{:< .4f}'.format(grid_search.best_estimator_.coef_[0][feature_index]), end = ' ') \n",
    "            print('{:< .4f}'.format(grid_search.best_estimator_.coef_[1][feature_index]), end = ' ') \n",
    "            print('{:< .4f}'.format(grid_search.best_estimator_.coef_[2][feature_index]), end = ' ') \n",
    "            print('{:< .4f}'.format(grid_search.best_estimator_.coef_[3][feature_index])) \n",
    "            \n",
    "build_table(CountVectorizer(token_pattern=r\"(?u)\\b\\w[\\w\\.(),]+\\b\",stop_words=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
